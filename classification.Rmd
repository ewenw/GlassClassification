Data from https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data
Attribute Information:
   1. Id number: 1 to 214
   2. RI: refractive index
   3. Na: Sodium (unit measurement: weight percent in corresponding oxide, as 
                  are attributes 4-10)
   4. Mg: Magnesium
   5. Al: Aluminum
   6. Si: Silicon
   7. K: Potassium
   8. Ca: Calcium
   9. Ba: Barium
  10. Fe: Iron
  11. Type of glass: (class attribute)
      -- 1 building_windows_float_processed
      -- 2 building_windows_non_float_processed
      -- 3 vehicle_windows_float_processed
      -- 4 vehicle_windows_non_float_processed (none in this database)
      -- 5 containers
      -- 6 tableware
      -- 7 headlamps
    
## Import data and impute outliers
```{r}
data <- read.csv(url("https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data"))
summary(data)
names(data) <- c('Id', 'RI', 'Na', 'Mg', 'Al' , 'Si', 'K', 'Ca', 'Ba', 'Fe', 'Type')


outlierImpute <- function(x) {
  x[x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x)] <- median(x)
}
lapply(data, outlierImpute)

data <- data[, -c(1)]
```


## Min-max normalization
```{r}

minMax <- function(x){
  min <- min(data[, 1])
  max <- max(data[, 1])
  x <- (x - min) / (max - x) 
}
data[, 1] <- minMax(data[, 1])
data[, 2] <- minMax(data[, 2])

```


## Z-score standardization
```{r}
zStandardize <- function(x){
  sigma <- sd(x)
  m <- mean(x)
  z <- (x - m) / sigma
}
names <- c(1:9)
data[, names] <- lapply(data[,names], zStandardize)
```


## Stratefied sampling from each class
```{r}
selectSample <- function(type, data, trainingRatio=0.5){
  rows <- data[data$Type == type,]
  if(nrow(rows) == 0)
    return(data)
  indices <- sample(1:nrow(rows), trainingRatio * nrow(rows), replace=F)
  rows[indices, ]$Use <- 'Training'
  rows[-indices, ]$Use <- 'Evaluation'
  data[data$Type == type,] <- rows
  return(data)
}
data$Use <- NA
for(i in 1:7){
  data <- selectSample(i, data, 0.5)
}
```


## k-NN Classfication
```{r}
euclideanDistance <- function(a, b){
  squareDiffs <- (a - b) ^ 2
  dist <- sqrt(sum(squareDiffs))
  return(dist)
}

# classify features vector from data's k nearest neighbors
classify <- function(features, k=11){
  trainingData <- data[data$Use=='Training',]
  #features <- do.call("rbind", replicate(nrow(trainingData), features, simplify = FALSE))
  trainingData$DistanceFromPoint <- apply(trainingData[,c(1:9)], 1, euclideanDistance, features)
  trainingData <- trainingData[order(trainingData$DistanceFromPoint),]
  selection <- trainingData[c(1:k),'Type']
  mode <- names(sort(-table(selection)))[1]
  return(mode)
}

classify(data[213,c(1:9)])

```


## Evaluation of accuracy
```{r}
accuracy <- function(k=11){
  evaluationData <- data[data$Use=='Evaluation',]
  evaluationData$ClassifiedType <- apply(evaluationData[, c(1:9)], 1, classify, k)
  corrects <- evaluationData[abs(as.numeric(evaluationData$ClassifiedType)-as.numeric(evaluationData$Type))<=1,]
  return(nrow(corrects) / nrow(evaluationData))
}

accuracy(10)
```


## Visualization of k (x-axis) versus accuracy
```{r}
accuracies <- data.frame('k'=c(5:12))
accuracies$Acc <- sapply(accuracies$k, accuracy)
plot(accuracies)
# k=8 has the best accuracy of 0.8532
```
